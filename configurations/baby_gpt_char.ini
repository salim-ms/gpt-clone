[Dataset]
dataset_name = shakespeare

[Tokenizer]
tokenizer_type = char

[Model]
model_type = baby_gpt
embed_size = 64
n_layers = 4
batch_size = 16
sequence_length = 32
n_heads = 4


[Training]
learning_rate = 1e-3
training_steps = 15000
eval_iterations = 200