[Dataset]
dataset_name = shakespeare

[Tokenizer]
tokenizer_type = char

[Model]
model_type = bigram
embed_size = 64
n_layers = 4
batch_size = 4
sequence_length = 8


[Training]
learning_rate = 1e-3
training_steps = 15000
eval_iterations = 200