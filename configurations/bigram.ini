[Dataset]
dataset_name = shakespeare

[Tokenizer]
tokenizer_type = char

[Model]
model_type = bigram
n_embed = 64
n_layers = 4
batch_size = 4
context_length = 8
training_steps = 15000
eval_iterations = 200